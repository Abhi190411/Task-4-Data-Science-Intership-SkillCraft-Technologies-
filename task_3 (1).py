# -*- coding: utf-8 -*-
"""Task 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-46Xp3rJ0nuG2HVaHhgmIW4myrHR2u64

Import Libraries
"""

# Import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

"""Load Data"""

from google.colab import files
uploaded = files.upload()

df = pd.read_csv("bank-additional-full.csv", sep=';', encoding='utf-8')

"""Drop leakage feature"""

# --- Step 3: Clean target column and engineer features ---

# Drop 'duration' if it exists (leakage variable)
if 'duration' in df.columns:
    df.drop(columns=['duration'], inplace=True)

# Ensure target column is string before cleaning
df['y'] = df['y'].astype(str).str.strip().str.lower()

# Map 'yes'/'no' to binary (1/0)
df['y'] = df['y'].map({'yes': 1, 'no': 0})

# Drop rows with NaN in target (in case of unexpected values)
df = df.dropna(subset=['y'])

# If 'pdays' exists, create was_contacted and clean pdays
if 'pdays' in df.columns:
    df['was_contacted'] = (df['pdays'] != 999).astype(int)
    df['pdays'] = df['pdays'].replace(999, np.nan)

# Identify categorical & numerical features
cat_feats = df.select_dtypes(include=['object']).columns.tolist()
num_feats = df.select_dtypes(include=['int64', 'float64']).columns.tolist()

# Remove target from both lists if present
if 'y' in cat_feats:
    cat_feats.remove('y')
if 'y' in num_feats:
    num_feats.remove('y')

print("Categorical Features:", cat_feats)
print("Numerical Features:", num_feats)
print("Data shape after Step 3:", df.shape)

"""Preprocessing pipeline"""

# Numerical transformer
num_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median'))
])

# Categorical transformer
cat_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),
    ('encoder', OneHotEncoder(handle_unknown='ignore'))
])

# ColumnTransformer for preprocessing
preprocessor = ColumnTransformer(transformers=[
    ('num', num_transformer, num_feats),
    ('cat', cat_transformer, cat_feats)
])

print(df['y'].unique())
print(df['y'].isnull().sum())

# Ensure y is treated as string first
df['y'] = df['y'].astype(str).str.strip().str.lower()

# Map to binary values, mapping anything not 'yes' or 'no' to NaN
df['y'] = df['y'].map({'yes': 1, 'no': 0})

# Drop rows with NaN in y (in case of unexpected values after mapping)
df = df.dropna(subset=['y'])

print(df['y'].value_counts())

# Reload dataset
df = pd.read_csv("bank-additional-full.csv", sep=';')

# Strip and lowercase
df['y'] = df['y'].str.strip().str.lower()

# Map to binary
df['y'] = df['y'].map({'yes': 1, 'no': 0})

# If there are still NaN in y, print them
if df['y'].isnull().sum() > 0:
    print("Unexpected y values found:")
    print(df.loc[df['y'].isnull(), 'y'])

print(df['y'].value_counts())

# Reload dataset
df = pd.read_csv("bank-additional-full.csv", sep=';')

# Clean target column
df['y'] = df['y'].astype(str).str.strip().str.lower()
df['y'] = df['y'].map({'yes': 1, 'no': 0})

# Create was_contacted from pdays
if 'pdays' in df.columns:
    df['was_contacted'] = (df['pdays'] != 999).astype(int)
    df['pdays'] = df['pdays'].replace(999, np.nan)

"""Train test split"""

X = df.drop(columns=['y'])
y = df['y']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, test_size=0.2, random_state=42
)

# Ensure DecisionTreeClassifier and other necessary components are imported
# (Run the first code cell 'S8dqfi_YG1e2' if you haven't already)

# Define the Decision Tree Classifier model
clf = DecisionTreeClassifier(
    criterion='gini',
    max_depth=None,           # fully grown tree
    min_samples_split=2,
    min_samples_leaf=1,
    random_state=42
)

# Create the full pipeline including preprocessing and the classifier
model = Pipeline(steps=[
    ('preprocessor', preprocessor), # Assuming 'preprocessor' is defined from previous steps
    ('classifier', clf)
])

# Train the model
model.fit(X_train, y_train) # Assuming X_train and y_train are defined from train_test_split

# ============================
# ✅ Model Evaluation
# ============================

# Predictions on test set
y_pred = model.predict(X_test)

# Classification Report
print("Classification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)

# ROC-AUC Score
print("ROC-AUC Score:", roc_auc_score(y_test, y_pred))

# Confusion Matrix Heatmap
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No','Yes'], yticklabels=['No','Yes'])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()


# ============================
# ✅ Visualize Decision Tree
# ============================

plt.figure(figsize=(20,10))
plot_tree(
    clf,
    filled=True,
    feature_names=model.named_steps['preprocessor'].get_feature_names_out(),
    class_names=['No', 'Yes'],
    max_depth=3  # limit depth for readability
)
plt.show()


# ============================
# ✅ Feature Importances
# ============================

importances = clf.feature_importances_
features = model.named_steps['preprocessor'].get_feature_names_out()

feat_importances = pd.Series(importances, index=features)
feat_importances.nlargest(15).plot(kind='barh', figsize=(10,6))
plt.title("Top 15 Feature Importances")
plt.show()


# ============================
# ✅ (Optional) Hyperparameter Tuning
# ============================

from sklearn.model_selection import GridSearchCV

param_grid = {
    'classifier__max_depth': [3, 5, 10, None],
    'classifier__min_samples_split': [2, 5, 10],
    'classifier__min_samples_leaf': [1, 2, 5]
}

grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1', n_jobs=-1)
grid_search.fit(X_train, y_train)

print("Best Parameters:", grid_search.best_params_)
print("Best F1 Score (CV):", grid_search.best_score_)

# Evaluate tuned model
y_pred_tuned = grid_search.best_estimator_.predict(X_test)
print("\nClassification Report (Tuned Model):\n", classification_report(y_test, y_pred_tuned))